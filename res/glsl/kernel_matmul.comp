// This compute shader performs a generic matrix multiply between two
// matrices A and B, returning a matrix C.
// 
// A has shape (MxK), B has shape (KxN) and C has shape (MxN).

#version 450

#include "nezha.glsl"

// These are placeholder values right now and are in the future to be
// initialized by the user who may require a specific shape for A and B
// as well as configuring the block count in each direction.
//
// The user will have to dispatch this compute shader with the following
// parameters:
//
// vkCmdDispatch(BLOCK_COUNT_N, BLOCK_COUNT_M, 1);
#define SHAPE_M       (64*3*10)
#define SHAPE_N       (32*3*40)
#define SHAPE_K       (100)

#define BLOCK_ITEMS_M (64)
#define BLOCK_ITEMS_N (32*2)
#define BLOCK_ITEMS_K (10)

// These are things which are derived from user defined values.
// We round up the block items constants.
#define BLOCK_COUNT_M ((SHAPE_M + BLOCK_ITEMS_M - 1) / BLOCK_ITEMS_M)
#define BLOCK_COUNT_N ((SHAPE_N + BLOCK_ITEMS_N - 1) / BLOCK_ITEMS_N)
#define BLOCK_COUNT_K ((SHAPE_K + BLOCK_ITEMS_K - 1) / BLOCK_ITEMS_K)

// Shared memory constants. We can precompute how much space is required
// for the output, the input A and the input B.
#define SM_OUTPUT_SIZE       (BLOCK_ITEMS_N * BLOCK_ITEMS_M)
#define SM_INPUT_A_TILE_SIZE (BLOCK_ITEMS_K * BLOCK_ITEMS_M)
#define SM_INPUT_B_TILE_SIZE (BLOCK_ITEMS_K * BLOCK_ITEMS_N)

// Some constants for warp level parallelism and configuration. Here,
// each warp in the M direction (Y direction) can process 64 floats
// and 32 floats in the N direction (X direction). This stems from the
// following calculation.
//
// We assume warp size is 32.
//
// Each warp can occupy a thread space of 8x4. In the M direction (with 8
// threads), we can process 2 vec4's -> 8 floats. In the N direction (with
// 4 threads), we can process 2 vec4's -> 8 floats. Therefore, we can
// process 64 floats in the M direction and 32 floats in the N direction.
#define WARP_SIZE 32
#define NUM_WARPS_M_PER_THREADGROUP (BLOCK_ITEMS_M / 64)
#define NUM_WARPS_N_PER_THREADGROUP (BLOCK_ITEMS_N / 32)
#define THREADS_PER_THREADGROUP (NUM_WARPS_M_PER_THREADGROUP * NUM_WARPS_N_PER_THREADGROUP * WARP_SIZE)

// Number of threads in each direction, within a threadgroup.
#define NUM_THREADS_M_PER_THREADGROUP (NUM_WARPS_M_PER_THREADGROUP * 8)
#define NUM_THREADS_N_PER_THREADGROUP (NUM_WARPS_N_PER_THREADGROUP * 4)

// We can calculate how many threads there are in each direction
// of a threadgroup by using the warp count calculated for a
// threadgroup.
layout (local_size_x = NUM_THREADS_N_PER_THREADGROUP,
        local_size_y = NUM_THREADS_M_PER_THREADGROUP, 
        local_size_z = 1) in;

// Global memory: the input matrices live here. This shader computes
// A (M x K) * B (K x N) = C (M x N).

// size: M * K stored in row-major order.
layout (set = 0, binding = 0) buffer in_mat_a 
{
  float data[]; 
} u_mat_a;

// size: K * N stored in row-major order.
layout (set = 1, binding = 0) buffer in_mat_b 
{
	float data[]; 
} u_mat_b;

// We have calculated how many floats from the inputs we need for the
// shared memory buffer.
shared struct 
{
  // This will contain the output of the tile matrix multiply.
  float output_tile[BLOCK_ITEMS_M * BLOCK_ITEMS_N];

  // These arrays in shared memory will contain the tiles corresponding to the
  // tile computation. These arrays will be used to accumulate output_tile.
  float a_tile[BLOCK_ITEMS_M * BLOCK_ITEMS_K]; // Contains a tile loaded from
                                               // matrix A (M x K).
  float b_tile[BLOCK_ITEMS_K * BLOCK_ITEMS_N]; // Contains a tile loaded from
                                               // matrix B (K x N).
} sm;

int collapse_2d(int x, int y, int row_len)
{
  return y * row_len + x;
}

// Here, we need to loop through each tile in the A and B matrices which correspond
// to the output tile we are trying to calculate right now in this threadgroup.
// The amount of times we need to perform the loop is BLOCK_COUNT_K.
void main()
{
  int m_start = int(gl_WorkGroupID.y) * BLOCK_ITEMS_M;
  int n_start = int(gl_WorkGroupID.x) * BLOCK_ITEMS_N;

  for (int k_i = 0; k_i < BLOCK_COUNT_K; ++k_i)
  {
    int k_start = k_i * BLOCK_ITEMS_K;

    // Copying into b_tile
    for (int y = int(gl_LocalInvocationID.y); y < BLOCK_ITEMS_K; y += NUM_THREADS_M_PER_THREADGROUP)
    {
      int local_idx_start_x   = int(gl_LocalInvocationID.x) * 8;
      int local_idx_end_x     = (int(gl_LocalInvocationID.x) + 1) * 8;

      for (int x = local_idx_start_x; x < local_idx_end_x; ++x)
      {
        sm.b_tile[collapse_2d(x, y, BLOCK_ITEMS_N)] = 
          u_mat_b.data[collapse_2d(x + n_start, y + k_start, SHAPE_N)];
      }
    }

    // Copying into a_tile
    for (int x = int(gl_LocalInvocationID.x); x < BLOCK_ITEMS_K; x += NUM_THREADS_N_PER_THREADGROUP)
    {
      int local_idx_start_y   = int(gl_LocalInvocationID.y) * 8;
      int local_idx_end_y     = (int(gl_LocalInvocationID.y) + 1) * 8;

      for (int y = local_idx_start_y; y < local_idx_end_y; ++y)
      {
        sm.a_tile[collapse_2d(x, y, BLOCK_ITEMS_K)] = 
          u_mat_a.data[collapse_2d(x + k_start, y + m_start, SHAPE_K)];
      }
    }
  }

  // Perform accumulation to the final matrix which lives in global memory.
  // TODO ...
}
