// This compute shader performs a generic matrix multiply between two
// matrices A and B, returning a matrix C.
// 
// A has shape (MxK), B has shape (KxN) and C has shape (MxN).

#version 450

#include "nezha.glsl"

// These are placeholder values right now and are in the future to be
// initialized by the user who may require a specific shape for A and B
// as well as configuring the block count in each direction.
//
// The user will have to dispatch this compute shader with the following
// parameters:
//
// vkCmdDispatch(BLOCK_COUNT_N, BLOCK_COUNT_M, 1);
#define SHAPE_M       (128)
#define SHAPE_N       (64)
#define SHAPE_K       (64)

#define BLOCK_ITEMS_M (64)
#define BLOCK_ITEMS_N (32)
#define BLOCK_ITEMS_K (4)

// These are things which are derived from user defined values.
// We round up the block items constants.
#define BLOCK_COUNT_M ((SHAPE_M + BLOCK_ITEMS_M - 1) / BLOCK_ITEMS_M)
#define BLOCK_COUNT_N ((SHAPE_N + BLOCK_ITEMS_N - 1) / BLOCK_ITEMS_N)
#define BLOCK_COUNT_K ((SHAPE_K + BLOCK_ITEMS_K - 1) / BLOCK_ITEMS_K)

// Shared memory constants. We can precompute how much space is required
// for the output, the input A and the input B.
#define SM_OUTPUT_SIZE       (BLOCK_ITEMS_N * BLOCK_ITEMS_M)
#define SM_INPUT_A_TILE_SIZE (BLOCK_ITEMS_K * BLOCK_ITEMS_M)
#define SM_INPUT_B_TILE_SIZE (BLOCK_ITEMS_K * BLOCK_ITEMS_N)

// Some constants for warp level parallelism and configuration. Here,
// each warp in the M direction (Y direction) can process 64 floats
// and 32 floats in the N direction (X direction). This stems from the
// following calculation.
//
// We assume warp size is 32.
//
// Each warp can occupy a thread space of 8x4. In the M direction (with 8
// threads), we can process 2 vec4's -> 8 floats. In the N direction (with
// 4 threads), we can process 2 vec4's -> 8 floats. Therefore, we can
// process 64 floats in the M direction and 32 floats in the N direction.
#define WARP_SIZE 32
#define NUM_WARPS_M_PER_THREADGROUP (BLOCK_ITEMS_M / 64)
#define NUM_WARPS_N_PER_THREADGROUP (BLOCK_ITEMS_N / 32)
#define THREADS_PER_THREADGROUP (NUM_WARPS_M_PER_THREADGROUP * NUM_WARPS_N_PER_THREADGROUP * WARP_SIZE)

// Number of threads in each direction, within a threadgroup.
#define NUM_THREADS_M_PER_THREADGROUP (NUM_WARPS_M_PER_THREADGROUP * 8)
#define NUM_THREADS_N_PER_THREADGROUP (NUM_WARPS_N_PER_THREADGROUP * 4)

#define NUM_THREADS_IN_WARP_M 8
#define NUM_THREADS_IN_WARP_N 4

// We can calculate how many threads there are in each direction
// of a threadgroup by using the warp count calculated for a
// threadgroup.
layout (local_size_x = NUM_THREADS_N_PER_THREADGROUP,
        local_size_y = NUM_THREADS_M_PER_THREADGROUP, 
        local_size_z = 1) in;

// Global memory: the input matrices live here. This shader computes
// A (M x K) * B (K x N) = C (M x N).

// size: M * K stored in row-major order.
layout (set = 0, binding = 0) buffer in_mat_a 
{
  float data[]; 
} u_mat_a;

// size: K * N stored in row-major order.
layout (set = 1, binding = 0) buffer in_mat_b 
{
	float data[]; 
} u_mat_b;

layout (set = 2, binding = 0) buffer out_mat
{
	float data[]; 
} u_mat_out;

// We have calculated how many floats from the inputs we need for the
// shared memory buffer.
shared struct 
{
  // This will contain the output of the tile matrix multiply.
  float output_tile[BLOCK_ITEMS_M * BLOCK_ITEMS_N];

  // These arrays in shared memory will contain the tiles corresponding to the
  // tile computation. These arrays will be used to accumulate output_tile.
  float a_tile[BLOCK_ITEMS_M * BLOCK_ITEMS_K]; // Contains a tile loaded from
                                               // matrix A (M x K).
  float b_tile[BLOCK_ITEMS_K * BLOCK_ITEMS_N]; // Contains a tile loaded from
                                               // matrix B (K x N).
} sm;

int collapse_2d(int x, int y, int row_len)
{
  return y * row_len + x;
}

void add_mat4_to_output(in mat4 m, int x_start, int y_start)
{
  for (int y = y_start; y < y_start + 4; ++y)
  {
    for (int x = x_start; x < x_start + 4; ++x)
    {
      sm.output_tile[collapse_2d(x, y, BLOCK_ITEMS_N)] += m[x-x_start][y-y_start];
    }
  }
}

void copy_8x8_to_global_output(int n_offset, int m_offset, int x_start, int y_start)
{
  for (int y = 0; y < 8; ++y)
  {
    for (int x = 0; x < 8; ++x)
    {
      u_mat_out.data[collapse_2d(n_offset + x_start + x, m_offset + y_start + y, SHAPE_N)] =
        sm.output_tile[collapse_2d(x_start + x, y_start + y, BLOCK_ITEMS_N)];
    }
  }
}

void copy_mat4_to_global_output(in mat4 m, int n_offset, int m_offset)
{
  for (int y = 0; y < 4; ++y)
  {
    for (int x = 0; x < 4; ++x)
    {
      u_mat_out.data[collapse_2d(n_offset + x, m_offset + y, SHAPE_N)] =
        m[x][y];
    }
  }
}

// Here, we need to loop through each tile in the A and B matrices which correspond
// to the output tile we are trying to calculate right now in this threadgroup.
// The amount of times we need to perform the loop is BLOCK_COUNT_K.
void main()
{
  int m_start = int(gl_WorkGroupID.y) * BLOCK_ITEMS_M;
  int n_start = int(gl_WorkGroupID.x) * BLOCK_ITEMS_N;

  int local_idx_start_x   = int(gl_LocalInvocationID.x) * 8;
  int local_idx_end_x     = (int(gl_LocalInvocationID.x) + 1) * 8;

  int local_idx_start_y   = int(gl_LocalInvocationID.y) * 8;
  int local_idx_end_y     = (int(gl_LocalInvocationID.y) + 1) * 8;

  int warp_local_thread_n = int(gl_LocalInvocationID.x) % 4;
  int warp_local_thread_m = int(gl_LocalInvocationID.y) % 8;

  int num_floats_n = NUM_THREADS_IN_WARP_N * 8;
  int num_floats_m = NUM_THREADS_IN_WARP_M * 8;

  int float_offset_m = warp_local_thread_m * 4 + num_floats_m * (int(gl_LocalInvocationID.y)/8);
  int float_offset_n = warp_local_thread_n * 4 + num_floats_n * (int(gl_LocalInvocationID.x)/4);

  mat4 res_a0b0 = mat4(0.0);
  mat4 res_a0b1 = mat4(0.0);
  mat4 res_a1b0 = mat4(0.0);
  mat4 res_a1b1 = mat4(0.0);

  for (int k_i = 0; k_i < BLOCK_COUNT_K; ++k_i)
  {
    int k_start = k_i * BLOCK_ITEMS_K;

    // Copying into b_tile
    for (int y = int(gl_LocalInvocationID.y); y < BLOCK_ITEMS_K; y += NUM_THREADS_M_PER_THREADGROUP)
    {
      for (int x = local_idx_start_x; x < local_idx_end_x; ++x)
      {
        sm.b_tile[collapse_2d(x, y, BLOCK_ITEMS_N)] = 
          u_mat_b.data[collapse_2d(x + n_start, y + k_start, SHAPE_N)];
      }
    }

    // Copying into a_tile
    for (int x = int(gl_LocalInvocationID.x); x < BLOCK_ITEMS_K; x += NUM_THREADS_N_PER_THREADGROUP)
    {
      for (int y = local_idx_start_y; y < local_idx_end_y; ++y)
      {
        sm.a_tile[collapse_2d(x, y, BLOCK_ITEMS_K)] = 
          u_mat_a.data[collapse_2d(x + k_start, y + m_start, SHAPE_K)];
      }
    }

    barrier();

    for (int item = 0; item < BLOCK_ITEMS_K; ++item)
    {
      // Top left corner
      int float_offset_k = item;

      vec4 a0 = vec4(sm.a_tile[collapse_2d(float_offset_k, float_offset_m + 0, BLOCK_ITEMS_K)],
                     sm.a_tile[collapse_2d(float_offset_k, float_offset_m + 1, BLOCK_ITEMS_K)],
                     sm.a_tile[collapse_2d(float_offset_k, float_offset_m + 2, BLOCK_ITEMS_K)],
                     sm.a_tile[collapse_2d(float_offset_k, float_offset_m + 3, BLOCK_ITEMS_K)]);

      vec4 a1 = vec4(sm.a_tile[collapse_2d(float_offset_k, float_offset_m + (num_floats_m/2) + 0, BLOCK_ITEMS_K)],
                     sm.a_tile[collapse_2d(float_offset_k, float_offset_m + (num_floats_m/2) + 1, BLOCK_ITEMS_K)],
                     sm.a_tile[collapse_2d(float_offset_k, float_offset_m + (num_floats_m/2) + 2, BLOCK_ITEMS_K)],
                     sm.a_tile[collapse_2d(float_offset_k, float_offset_m + (num_floats_m/2) + 3, BLOCK_ITEMS_K)]);

      vec4 b0 = vec4(sm.b_tile[collapse_2d(float_offset_n + 0, float_offset_k, BLOCK_ITEMS_N)],
                     sm.b_tile[collapse_2d(float_offset_n + 1, float_offset_k, BLOCK_ITEMS_N)],
                     sm.b_tile[collapse_2d(float_offset_n + 2, float_offset_k, BLOCK_ITEMS_N)],
                     sm.b_tile[collapse_2d(float_offset_n + 3, float_offset_k, BLOCK_ITEMS_N)]);

      vec4 b1 = vec4(sm.b_tile[collapse_2d(float_offset_n + (num_floats_n/2) + 0, float_offset_k, BLOCK_ITEMS_N)],
                     sm.b_tile[collapse_2d(float_offset_n + (num_floats_n/2) + 1, float_offset_k, BLOCK_ITEMS_N)],
                     sm.b_tile[collapse_2d(float_offset_n + (num_floats_n/2) + 2, float_offset_k, BLOCK_ITEMS_N)],
                     sm.b_tile[collapse_2d(float_offset_n + (num_floats_n/2) + 3, float_offset_k, BLOCK_ITEMS_N)]);

      res_a0b0 += outerProduct(a0, b0);
      res_a0b1 += outerProduct(a0, b1);

      res_a1b0 += outerProduct(a1, b0);
      res_a1b1 += outerProduct(a1, b1);
    }

    barrier();
  }

  copy_mat4_to_global_output(res_a0b0, n_start + float_offset_n, m_start + float_offset_m);
  copy_mat4_to_global_output(res_a0b1, n_start + float_offset_n + (num_floats_n/2), m_start + float_offset_m);
  copy_mat4_to_global_output(res_a1b0, n_start + float_offset_n, m_start + float_offset_m + (num_floats_m/2));
  copy_mat4_to_global_output(res_a1b1, n_start + float_offset_n + (num_floats_n/2), m_start + float_offset_m + (num_floats_m/2));
}
