// This compute shader performs a generic matrix multiply between two
// matrices A and B, returning a matrix C.
// 
// A has shape (MxK), B has shape (KxN) and C has shape (MxN).

#version 450

#include "nezha.glsl"

#if 1
const uint SHAPE_M     = (640*640*3);
const uint SHAPE_N     = (32);
const uint SHAPE_K     = (32*3);
#else
const uint SHAPE_M     = (128*128);
const uint SHAPE_N     = (128*128);
const uint SHAPE_K     = (128*128);
#endif

const uint BLOCK_ITEMS_M = (64);
const uint BLOCK_ITEMS_N = (32);
const uint BLOCK_ITEMS_K = (8); // 4 IS OPTIMAL

// These are things which are derived from user defined values.
// We round up the block items constants.
const uint BLOCK_COUNT_M = ((SHAPE_M + BLOCK_ITEMS_M - 1) / BLOCK_ITEMS_M);
const uint BLOCK_COUNT_N = ((SHAPE_N + BLOCK_ITEMS_N - 1) / BLOCK_ITEMS_N);
const uint BLOCK_COUNT_K = ((SHAPE_K + BLOCK_ITEMS_K - 1) / BLOCK_ITEMS_K);

// Shared memory constants. We can precompute how much space is required
// for the output, the input A and the input B.
const uint SM_OUTPUT_SIZE       = (BLOCK_ITEMS_N * BLOCK_ITEMS_M);
const uint SM_INPUT_A_TILE_SIZE = (BLOCK_ITEMS_K * BLOCK_ITEMS_M);
const uint SM_INPUT_B_TILE_SIZE = (BLOCK_ITEMS_K * BLOCK_ITEMS_N);

// Some constants for warp level parallelism and configuration. Here,
// each warp in the M direction (Y direction) can process 64 floats
// and 32 floats in the N direction (X direction). This stems from the
// following calculation.
//
// We assume warp size is 32.
//
// Each warp can occupy a thread space of 8x4. In the M direction (with 8
// threads), we can process 2 vec4's -> 8 floats. In the N direction (with
// 4 threads), we can process 2 vec4's -> 8 floats. Therefore, we can
// process 64 floats in the M direction and 32 floats in the N direction.
const uint WARP_SIZE                   = 32;
const uint NUM_WARPS_M_PER_THREADGROUP = (BLOCK_ITEMS_M / 32);
const uint NUM_WARPS_N_PER_THREADGROUP = (BLOCK_ITEMS_N / 16);
const uint THREADS_PER_THREADGROUP     = (NUM_WARPS_M_PER_THREADGROUP * NUM_WARPS_N_PER_THREADGROUP * WARP_SIZE);

// Number of threads in each direction, within a threadgroup.
const uint NUM_THREADS_M_PER_THREADGROUP = (NUM_WARPS_M_PER_THREADGROUP * 8);
const uint NUM_THREADS_N_PER_THREADGROUP = (NUM_WARPS_N_PER_THREADGROUP * 4);

const uint NUM_THREADS_IN_WARP_M = 8;
const uint NUM_THREADS_IN_WARP_N = 4;

// We can calculate how many threads there are in each direction
// of a threadgroup by using the warp count calculated for a
// threadgroup.
layout (local_size_x = NUM_THREADS_N_PER_THREADGROUP,
        local_size_y = NUM_THREADS_M_PER_THREADGROUP, 
        local_size_z = 1) in;

// Global memory: the input matrices live here. This shader computes
// A (M x K) * B (K x N) = C (M x N).

// size: M * K stored in row-major order.
layout (set = 0, binding = 0) buffer in_mat_a 
{
  float data[]; 
} u_mat_a;

// size: K * N stored in row-major order.
layout (set = 1, binding = 0) buffer in_mat_b 
{
	float data[]; 
} u_mat_b;

layout (set = 2, binding = 0) buffer out_mat
{
	float data[]; 
} u_mat_out;

// We have calculated how many floats from the inputs we need for the
// shared memory buffer.
shared struct 
{
  // This will contain the output of the tile matrix multiply.
  float output_tile[BLOCK_ITEMS_M * BLOCK_ITEMS_N];

  // These arrays in shared memory will contain the tiles corresponding to the
  // tile computation. These arrays will be used to accumulate output_tile.
  // float a_tile[BLOCK_ITEMS_M * BLOCK_ITEMS_K]; // Contains a tile loaded from
  float a_tile[BLOCK_ITEMS_K * BLOCK_ITEMS_M];
                                               // matrix A (M x K).
  float b_tile[BLOCK_ITEMS_K * BLOCK_ITEMS_N]; // Contains a tile loaded from
                                               // matrix B (K x N).
} sm;

#if 1
uint collapse_2d(uint x, uint y, uint row_len)
{
  return y * row_len + x;
}
#endif

// #define collapse_2d(x, y, row_len) ((y) * (row_len) + (x))

void add_mat4_to_output(in mat4 m, uint x_start, uint y_start)
{
  for (uint y = y_start; y < y_start + 4; ++y)
  {
    for (uint x = x_start; x < x_start + 4; ++x)
    {
      sm.output_tile[collapse_2d(x, y, BLOCK_ITEMS_N)] += m[x-x_start][y-y_start];
    }
  }
}

void copy_8x8_to_global_output(uint n_offset, uint m_offset, uint x_start, uint y_start)
{
  for (uint y = 0; y < 8; ++y)
  {
    for (uint x = 0; x < 8; ++x)
    {
      u_mat_out.data[collapse_2d(n_offset + x_start + x, m_offset + y_start + y, SHAPE_N)] =
        sm.output_tile[collapse_2d(x_start + x, y_start + y, BLOCK_ITEMS_N)];
    }
  }
}

void copy_mat4_to_global_output(in mat4 m, uint n_offset, uint m_offset)
{
  for (uint y = 0; y < 4; ++y)
  {
    for (uint x = 0; x < 4; ++x)
    {
      u_mat_out.data[collapse_2d(n_offset + x, m_offset + y, SHAPE_N)] =
        m[x][y];
    }
  }
}

// Here, we need to loop through each tile in the A and B matrices which correspond
// to the output tile we are trying to calculate right now in this threadgroup.
// The amount of times we need to perform the loop is BLOCK_COUNT_K.
void main()
{
  uint m_start = gl_WorkGroupID.y * BLOCK_ITEMS_M;
  uint n_start = gl_WorkGroupID.x * BLOCK_ITEMS_N;

  uint local_idx_start_x   = gl_LocalInvocationID.x * 8;
  uint local_idx_end_x     = (gl_LocalInvocationID.x + 1) * 8;

  uint local_idx_start_y   = gl_LocalInvocationID.y * 8;
  uint local_idx_end_y     = (gl_LocalInvocationID.y + 1) * 8;

  uint warp_local_thread_n = gl_LocalInvocationID.x % 4;
  uint warp_local_thread_m = gl_LocalInvocationID.y % 8;

  uint num_floats_n = NUM_THREADS_IN_WARP_N * 8;
  uint num_floats_m = NUM_THREADS_IN_WARP_M * 8;

  uint float_offset_m = warp_local_thread_m * 4 + num_floats_m * (gl_LocalInvocationID.y/8);
  uint float_offset_n = warp_local_thread_n * 4 + num_floats_n * (gl_LocalInvocationID.x/4);

  mat4 res_a0b0 = mat4(0.0);
  mat4 res_a0b1 = mat4(0.0);
  mat4 res_a1b0 = mat4(0.0);
  mat4 res_a1b1 = mat4(0.0);

  for (uint k_i = 0; k_i < BLOCK_COUNT_K; ++k_i)
  {
    uint k_start = k_i * BLOCK_ITEMS_K;

    // Copying uinto b_tile
    for (uint y = gl_LocalInvocationID.y; y < BLOCK_ITEMS_K; y += NUM_THREADS_M_PER_THREADGROUP)
    {
      for (uint x = local_idx_start_x; x < local_idx_end_x; ++x)
      {
        sm.b_tile[collapse_2d(x, y, BLOCK_ITEMS_N)] = 
          u_mat_b.data[collapse_2d(x + n_start, y + k_start, SHAPE_N)];
      }
    }

    // Copying uinto a_tile
    for (uint x = gl_LocalInvocationID.x; x < BLOCK_ITEMS_K; x += NUM_THREADS_N_PER_THREADGROUP)
    {
      for (uint y = local_idx_start_y; y < local_idx_end_y; ++y)
      {
        sm.a_tile[collapse_2d(y, x, BLOCK_ITEMS_M)] = 
          u_mat_a.data[collapse_2d(y + m_start, x + k_start, SHAPE_M)];
      }
    }

    barrier();

    for (uint item = 0; item < BLOCK_ITEMS_K; ++item)
    {
      // Top left corner
      uint float_offset_k = item;

      vec4 a0 = vec4(sm.a_tile[collapse_2d(float_offset_m + 0, float_offset_k, BLOCK_ITEMS_M)],
                     sm.a_tile[collapse_2d(float_offset_m + 1, float_offset_k, BLOCK_ITEMS_M)],
                     sm.a_tile[collapse_2d(float_offset_m + 2, float_offset_k, BLOCK_ITEMS_M)],
                     sm.a_tile[collapse_2d(float_offset_m + 3, float_offset_k, BLOCK_ITEMS_M)]);

      vec4 a1 = vec4(sm.a_tile[collapse_2d(float_offset_m + (num_floats_m/2) + 0, float_offset_k, BLOCK_ITEMS_M)],
                     sm.a_tile[collapse_2d(float_offset_m + (num_floats_m/2) + 1, float_offset_k, BLOCK_ITEMS_M)],
                     sm.a_tile[collapse_2d(float_offset_m + (num_floats_m/2) + 2, float_offset_k, BLOCK_ITEMS_M)],
                     sm.a_tile[collapse_2d(float_offset_m + (num_floats_m/2) + 3, float_offset_k, BLOCK_ITEMS_M)]);

      vec4 b0 = vec4(sm.b_tile[collapse_2d(float_offset_n + 0, float_offset_k, BLOCK_ITEMS_N)],
                     sm.b_tile[collapse_2d(float_offset_n + 1, float_offset_k, BLOCK_ITEMS_N)],
                     sm.b_tile[collapse_2d(float_offset_n + 2, float_offset_k, BLOCK_ITEMS_N)],
                     sm.b_tile[collapse_2d(float_offset_n + 3, float_offset_k, BLOCK_ITEMS_N)]);

      vec4 b1 = vec4(sm.b_tile[collapse_2d(float_offset_n + (num_floats_n/2) + 0, float_offset_k, BLOCK_ITEMS_N)],
                     sm.b_tile[collapse_2d(float_offset_n + (num_floats_n/2) + 1, float_offset_k, BLOCK_ITEMS_N)],
                     sm.b_tile[collapse_2d(float_offset_n + (num_floats_n/2) + 2, float_offset_k, BLOCK_ITEMS_N)],
                     sm.b_tile[collapse_2d(float_offset_n + (num_floats_n/2) + 3, float_offset_k, BLOCK_ITEMS_N)]);

      res_a0b0 += outerProduct(a0, b0);
      res_a0b1 += outerProduct(a0, b1);

      res_a1b0 += outerProduct(a1, b0);
      res_a1b1 += outerProduct(a1, b1);
    }

    barrier();
  }

  copy_mat4_to_global_output(res_a0b0, n_start + float_offset_n, m_start + float_offset_m);
  copy_mat4_to_global_output(res_a0b1, n_start + float_offset_n + (num_floats_n/2), m_start + float_offset_m);
  copy_mat4_to_global_output(res_a1b0, n_start + float_offset_n, m_start + float_offset_m + (num_floats_m/2));
  copy_mat4_to_global_output(res_a1b1, n_start + float_offset_n + (num_floats_n/2), m_start + float_offset_m + (num_floats_m/2));
}
