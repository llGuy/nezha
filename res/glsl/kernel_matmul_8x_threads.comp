// This compute shader performs a generic matrix multiply between two
// matrices A and B, returning a matrix C.
// 
// A has shape (MxK), B has shape (KxN) and C has shape (MxN).

#version 450

#include "nezha.glsl"

// These are placeholder values right now and are in the future to be
// initialized by the user who may require a specific shape for A and B
// as well as configuring the block count in each direction.
//
// The user will have to dispatch this compute shader with the following
// parameters:
//
// vkCmdDispatch(BLOCK_COUNT_N, BLOCK_COUNT_M, 1);


const int SHAPE_M     = (640*640*3);
const int SHAPE_N     = (32);
const int SHAPE_K     = (32*3);


const int BLOCK_ITEMS_M = (64);
const int BLOCK_ITEMS_N = (32);
const int BLOCK_ITEMS_K = (4); // 4 IS OPTIMAL

// These are things which are derived from user defined values.
// We round up the block items constants.
const int BLOCK_COUNT_M = ((SHAPE_M + BLOCK_ITEMS_M - 1) / BLOCK_ITEMS_M);
const int BLOCK_COUNT_N = ((SHAPE_N + BLOCK_ITEMS_N - 1) / BLOCK_ITEMS_N);
const int BLOCK_COUNT_K = ((SHAPE_K + BLOCK_ITEMS_K - 1) / BLOCK_ITEMS_K);

// Shared memory constants. We can precompute how much space is required
// for the output, the input A and the input B.
const int SM_OUTPUT_SIZE       = (BLOCK_ITEMS_N * BLOCK_ITEMS_M);
const int SM_INPUT_A_TILE_SIZE = (BLOCK_ITEMS_K * BLOCK_ITEMS_M);
const int SM_INPUT_B_TILE_SIZE = (BLOCK_ITEMS_K * BLOCK_ITEMS_N);

// Some constants for warp level parallelism and configuration. Here,
// each warp in the M direction (Y direction) can process 64 floats
// and 32 floats in the N direction (X direction). This stems from the
// following calculation.
//
// We assume warp size is 32.
//
// Each warp can occupy a thread space of 8x4. In the M direction (with 8
// threads), we can process 2 vec4's -> 8 floats. In the N direction (with
// 4 threads), we can process 2 vec4's -> 8 floats. Therefore, we can
// process 64 floats in the M direction and 32 floats in the N direction.
const int WARP_SIZE                   = 32;
const int NUM_WARPS_M_PER_THREADGROUP = (BLOCK_ITEMS_M / 16);
const int NUM_WARPS_N_PER_THREADGROUP = (BLOCK_ITEMS_N / 8);
const int THREADS_PER_THREADGROUP     = (NUM_WARPS_M_PER_THREADGROUP * NUM_WARPS_N_PER_THREADGROUP * WARP_SIZE);

// Number of threads in each direction, within a threadgroup.
const int NUM_THREADS_M_PER_THREADGROUP = (NUM_WARPS_M_PER_THREADGROUP * 8);
const int NUM_THREADS_N_PER_THREADGROUP = (NUM_WARPS_N_PER_THREADGROUP * 4);

const int NUM_THREADS_IN_WARP_M = 8;
const int NUM_THREADS_IN_WARP_N = 4;

// We can calculate how many threads there are in each direction
// of a threadgroup by using the warp count calculated for a
// threadgroup.
layout (local_size_x = NUM_THREADS_N_PER_THREADGROUP,
        local_size_y = NUM_THREADS_M_PER_THREADGROUP, 
        local_size_z = 1) in;

// Global memory: the input matrices live here. This shader computes
// A (M x K) * B (K x N) = C (M x N).

// size: M * K stored in row-major order.
layout (set = 0, binding = 0) buffer in_mat_a 
{
  float data[]; 
} u_mat_a;

// size: K * N stored in row-major order.
layout (set = 1, binding = 0) buffer in_mat_b 
{
	float data[]; 
} u_mat_b;

layout (set = 2, binding = 0) buffer out_mat
{
	float data[]; 
} u_mat_out;

// We have calculated how many floats from the inputs we need for the
// shared memory buffer.
shared struct 
{
  // This will contain the output of the tile matrix multiply.
  float output_tile[BLOCK_ITEMS_M][BLOCK_ITEMS_N];

  // These arrays in shared memory will contain the tiles corresponding to the
  // tile computation. These arrays will be used to accumulate output_tile.
  // float a_tile[BLOCK_ITEMS_M][BLOCK_ITEMS_K]; // Contains a tile loaded from
  float a_tile[BLOCK_ITEMS_K][BLOCK_ITEMS_M];
                                               // matrix A (M x K).
  // float b_tile[BLOCK_ITEMS_K][BLOCK_ITEMS_N]; // Contains a tile loaded from
  float b_tile[BLOCK_ITEMS_N][BLOCK_ITEMS_K];
                                               // matrix B (K x N).
} sm;

#define collapse_2d(x, y, row_len) ((x) + (y) * (row_len))

void copy_mat4_to_global_output(in mat4 m, int n_offset, int m_offset)
{
  for (int y = 0; y < 4; ++y)
  {
    for (int x = 0; x < 4; ++x)
    {
      u_mat_out.data[collapse_2d(n_offset + x, m_offset + y, SHAPE_N)] = m[x][y];
    }
  }
}

void copy_mat2_to_global_output(in mat2 m, int n_offset, int m_offset)
{
  for (int y = 0; y < 2; ++y)
  {
    for (int x = 0; x < 2; ++x)
    {
      u_mat_out.data[collapse_2d(n_offset + x, m_offset + y, SHAPE_N)] = m[x][y];
    }
  }
}

// Here, we need to loop through each tile in the A and B matrices which correspond
// to the output tile we are trying to calculate right now in this threadgroup.
// The amount of times we need to perform the loop is BLOCK_COUNT_K.
void main()
{
  // The start coordinate of the output data
  int m_start = int(gl_WorkGroupID.y) * BLOCK_ITEMS_M;
  int n_start = int(gl_WorkGroupID.x) * BLOCK_ITEMS_N;

  // Local start we multiply the local invocation id by 4 because each
  // thread is responsible for 2 floats in each direction.
  int local_idx_start_x   = int(gl_LocalInvocationID.x) * 2;
  int local_idx_start_y   = int(gl_LocalInvocationID.y) * 2;

  // Get the x and y coordinate of the thread in its warp.
  int warp_local_thread_n = int(gl_LocalInvocationID.x) % 4;
  int warp_local_thread_m = int(gl_LocalInvocationID.y) % 8;

  // The number of floats operated on by the warp in each direction.
  int num_floats_n = NUM_THREADS_IN_WARP_N * 2;
  int num_floats_m = NUM_THREADS_IN_WARP_M * 2;

  // The float offset of the first that the entire warp will operate on in the BLOCK ITEMS.
  int float_offset_m = warp_local_thread_m * 2 + num_floats_m * (int(gl_LocalInvocationID.y)/8);
  int float_offset_n = warp_local_thread_n * 2 + num_floats_n * (int(gl_LocalInvocationID.x)/4);

  mat2 res_a0b0 = mat2(0.0);

  for (int k_i = 0; k_i < BLOCK_COUNT_K; ++k_i)
  {
    int k_start = k_i * BLOCK_ITEMS_K;

    // Copying into b_tile
    for (int y = int(gl_LocalInvocationID.y); y < BLOCK_ITEMS_K; y += NUM_THREADS_M_PER_THREADGROUP)
    {
      int tmp_y_off = local_idx_start_x + n_start + (y+k_start)*SHAPE_N;
#if 0
      sm.b_tile[y][local_idx_start_x + 0] = u_mat_b.data[collapse_2d(local_idx_start_x + 0 + n_start, y + k_start, SHAPE_N)];
      sm.b_tile[y][local_idx_start_x + 1] = u_mat_b.data[collapse_2d(local_idx_start_x + 1 + n_start, y + k_start, SHAPE_N)];
      sm.b_tile[y][local_idx_start_x + 2] = u_mat_b.data[collapse_2d(local_idx_start_x + 2 + n_start, y + k_start, SHAPE_N)];
      sm.b_tile[y][local_idx_start_x + 3] = u_mat_b.data[collapse_2d(local_idx_start_x + 3 + n_start, y + k_start, SHAPE_N)];
#endif
      sm.b_tile[y][local_idx_start_x + 0] = u_mat_b.data[0 + tmp_y_off];
      sm.b_tile[y][local_idx_start_x + 1] = u_mat_b.data[1 + tmp_y_off];
      // sm.b_tile[y][local_idx_start_x + 2] = u_mat_b.data[2 + tmp_y_off];
      // sm.b_tile[y][local_idx_start_x + 3] = u_mat_b.data[3 + tmp_y_off];
    }

    // Copying into a_tile - column major
    for (int x = int(gl_LocalInvocationID.x); x < BLOCK_ITEMS_K; x += NUM_THREADS_N_PER_THREADGROUP)
    {
      int tmp_x = (x+k_start)*SHAPE_M + (local_idx_start_y + m_start);
      sm.a_tile[x][local_idx_start_y + 0] = u_mat_a.data[tmp_x + 0];
      sm.a_tile[x][local_idx_start_y + 1] = u_mat_a.data[tmp_x + 1];

      // sm.a_tile[local_idx_start_y + 2][x] = u_mat_a.data[tmp_x + SHAPE_K*2];
      // sm.a_tile[local_idx_start_y + 3][x] = u_mat_a.data[tmp_x + SHAPE_K*3];
    }

    barrier();

    for (int item = 0; item < BLOCK_ITEMS_K; ++item)
    {
      // Top left corner
      int float_offset_k = item;

      vec2 a0 = vec2(sm.a_tile[float_offset_k][float_offset_m + 0],
                     sm.a_tile[float_offset_k][float_offset_m + 1]);

      vec2 b0 = vec2(sm.b_tile[float_offset_k][float_offset_n + 0],
                     sm.b_tile[float_offset_k][float_offset_n + 1]);

      res_a0b0 += outerProduct(a0, b0);
    }

    barrier();
  }

  copy_mat2_to_global_output(res_a0b0, n_start + float_offset_n, m_start + float_offset_m);
}
