// This compute shader performs a generic matrix multiply between two
// matrices A and B, returning a matrix C.
// 
// A has shape (MxK), B has shape (KxN) and C has shape (MxN).

#version 450

#include "nezha.glsl"

// These are placeholder values right now and are in the future to be
// initialized by the user who may require a specific shape for A and B
// as well as configuring the block count in each direction.
//
// The user will have to dispatch this compute shader with the following
// parameters:
//
// vkCmdDispatch(BLOCK_COUNT_N, BLOCK_COUNT_M, 1);


const uint SHAPE_M     = (640*640*3);
const uint SHAPE_N     = (32);
const uint SHAPE_K     = (32*3);


const uint BLOCK_ITEMS_M = (64);
const uint BLOCK_ITEMS_N = (32);
const uint BLOCK_ITEMS_K = (8); // 4 IS OPTIMAL

// These are things which are derived from user defined values.
// We round up the block items constants.
const uint BLOCK_COUNT_M = ((SHAPE_M + BLOCK_ITEMS_M - 1) / BLOCK_ITEMS_M);
const uint BLOCK_COUNT_N = ((SHAPE_N + BLOCK_ITEMS_N - 1) / BLOCK_ITEMS_N);
const uint BLOCK_COUNT_K = ((SHAPE_K + BLOCK_ITEMS_K - 1) / BLOCK_ITEMS_K);

// Shared memory constants. We can precompute how much space is required
// for the output, the input A and the input B.
const uint SM_OUTPUT_SIZE       = (BLOCK_ITEMS_N * BLOCK_ITEMS_M);
const uint SM_INPUT_A_TILE_SIZE = (BLOCK_ITEMS_K * BLOCK_ITEMS_M);
const uint SM_INPUT_B_TILE_SIZE = (BLOCK_ITEMS_K * BLOCK_ITEMS_N);

// Some constants for warp level parallelism and configuration. Here,
// each warp in the M direction (Y direction) can process 64 floats
// and 32 floats in the N direction (X direction). This stems from the
// following calculation.
//
// We assume warp size is 32.
//
// Each warp can occupy a thread space of 8x4. In the M direction (with 8
// threads), we can process 2 vec4's -> 8 floats. In the N direction (with
// 4 threads), we can process 2 vec4's -> 8 floats. Therefore, we can
// process 64 floats in the M direction and 32 floats in the N direction.
const uint WARP_SIZE                   = 32;
const uint NUM_WARPS_M_PER_THREADGROUP = (BLOCK_ITEMS_M / 32);
const uint NUM_WARPS_N_PER_THREADGROUP = (BLOCK_ITEMS_N / 16);
const uint THREADS_PER_THREADGROUP     = (NUM_WARPS_M_PER_THREADGROUP * NUM_WARPS_N_PER_THREADGROUP * WARP_SIZE);

// Number of threads in each direction, within a threadgroup.
const uint NUM_THREADS_M_PER_THREADGROUP = (NUM_WARPS_M_PER_THREADGROUP * 8);
const uint NUM_THREADS_N_PER_THREADGROUP = (NUM_WARPS_N_PER_THREADGROUP * 4);

const uint NUM_THREADS_IN_WARP_M = 8;
const uint NUM_THREADS_IN_WARP_N = 4;

// We can calculate how many threads there are in each direction
// of a threadgroup by using the warp count calculated for a
// threadgroup.
layout (local_size_x = NUM_THREADS_N_PER_THREADGROUP,
        local_size_y = NUM_THREADS_M_PER_THREADGROUP, 
        local_size_z = 1) in;

// Global memory: the input matrices live here. This shader computes
// A (M x K) * B (K x N) = C (M x N).

// size: M * K stored in row-major order.
layout (set = 0, binding = 0) buffer in_mat_a 
{
  float data[]; 
} u_mat_a;

// size: K * N stored in row-major order.
layout (set = 1, binding = 0) buffer in_mat_b 
{
	float data[]; 
} u_mat_b;

layout (set = 2, binding = 0) buffer out_mat
{
	float data[]; 
} u_mat_out;

// We have calculated how many floats from the inputs we need for the
// shared memory buffer.
shared struct 
{
  // This will contain the output of the tile matrix multiply.
  float output_tile[BLOCK_ITEMS_M][BLOCK_ITEMS_N];

  // These arrays in shared memory will contain the tiles corresponding to the
  // tile computation. These arrays will be used to accumulate output_tile.
  //float a_tile[BLOCK_ITEMS_M][BLOCK_ITEMS_K]; // Contains a tile loaded from
  float a_tile[BLOCK_ITEMS_K][BLOCK_ITEMS_M];
                                               // matrix A (M x K).
  float b_tile[BLOCK_ITEMS_K][BLOCK_ITEMS_N]; // Contains a tile loaded from
                                               // matrix B (K x N).
} sm;

#define collapse_2d(x, y, row_len) ((x) + (y) * (row_len))

void copy_mat4_to_global_output(in mat4 m, uint n_offset, uint m_offset)
{
  for (uint y = 0; y < 4; ++y)
  {
    for (uint x = 0; x < 4; ++x)
    {
      u_mat_out.data[collapse_2d(n_offset + x, m_offset + y, SHAPE_N)] = m[x][y];
    }
  }
}

const int NUM_LOADS_PER_THREAD_ITER = 4;

// Here, we need to loop through each tile in the A and B matrices which correspond
// to the output tile we are trying to calculate right now in this threadgroup.
// The amount of times we need to perform the loop is BLOCK_COUNT_K.
void main()
{
  // The start coordinate of the output data
  uint m_start = gl_WorkGroupID.y * BLOCK_ITEMS_M;
  uint n_start = gl_WorkGroupID.x * BLOCK_ITEMS_N;

  // Local start we multiply the local invocation id by 4 because each
  // thread is responsible for 4 floats in each direction.
  uint local_idx_start_x   = gl_LocalInvocationID.x * 4;
  uint local_idx_start_y   = gl_LocalInvocationID.y * 4;

  // Get the x and y coordinate of the thread in its warp.
  uint warp_local_thread_n = gl_LocalInvocationID.x % 4;
  uint warp_local_thread_m = gl_LocalInvocationID.y % 8;

  // The number of floats operated on by the warp in each direction.
  uint num_floats_n = NUM_THREADS_IN_WARP_N * 4;
  uint num_floats_m = NUM_THREADS_IN_WARP_M * 4;

  // The float offset of the first that the entire warp will operate on in the BLOCK ITEMS.
  uint float_offset_m = warp_local_thread_m * 4 + num_floats_m * (gl_LocalInvocationID.y/8);
  uint float_offset_n = warp_local_thread_n * 4 + num_floats_n * (gl_LocalInvocationID.x/4);

  mat4 res_a0b0 = mat4(0.0);

  for (uint k_i = 0; k_i < BLOCK_COUNT_K; ++k_i)
  {
    uint k_start = k_i * BLOCK_ITEMS_K;

    // Copying uinto b_tile
    for (uint y = gl_LocalInvocationID.y; y < BLOCK_ITEMS_K; y += NUM_THREADS_M_PER_THREADGROUP)
    {
      uint tmp_y_off = local_idx_start_x + n_start + (y+k_start)*SHAPE_N;
      sm.b_tile[y][local_idx_start_x + 0] = u_mat_b.data[0 + tmp_y_off];
      sm.b_tile[y][local_idx_start_x + 1] = u_mat_b.data[1 + tmp_y_off];
      sm.b_tile[y][local_idx_start_x + 2] = u_mat_b.data[2 + tmp_y_off];
      sm.b_tile[y][local_idx_start_x + 3] = u_mat_b.data[3 + tmp_y_off];
    }

    // Copying uinto a_tile - column major
    for (uint x = gl_LocalInvocationID.x; x < BLOCK_ITEMS_K; x += NUM_THREADS_N_PER_THREADGROUP)
    {
      uint tmp_x = (x+k_start)*SHAPE_M + (local_idx_start_y + m_start);
      sm.a_tile[x][local_idx_start_y + 0] = u_mat_a.data[tmp_x + 0];
      sm.a_tile[x][local_idx_start_y + 1] = u_mat_a.data[tmp_x + 1];
      sm.a_tile[x][local_idx_start_y + 2] = u_mat_a.data[tmp_x + 2];
      sm.a_tile[x][local_idx_start_y + 3] = u_mat_a.data[tmp_x + 3];
    }

    barrier();

    for (uint item = 0; item < BLOCK_ITEMS_K; ++item)
    {
      // Top left corner
      uint float_offset_k = item;

      vec4 a0 = vec4(sm.a_tile[float_offset_k][float_offset_m + 0],
                     sm.a_tile[float_offset_k][float_offset_m + 1],
                     sm.a_tile[float_offset_k][float_offset_m + 2],
                     sm.a_tile[float_offset_k][float_offset_m + 3]);

      vec4 b0 = vec4(sm.b_tile[float_offset_k][float_offset_n + 0],
                     sm.b_tile[float_offset_k][float_offset_n + 1],
                     sm.b_tile[float_offset_k][float_offset_n + 2],
                     sm.b_tile[float_offset_k][float_offset_n + 3]);

      res_a0b0 += outerProduct(a0, b0);
    }

    barrier();
  }

  copy_mat4_to_global_output(res_a0b0, n_start + float_offset_n, m_start + float_offset_m);
}
